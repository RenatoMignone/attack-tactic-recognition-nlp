{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "996562b2",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5919201a",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afbe41d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.12.12\n",
      "Requirement already satisfied: pip in /usr/local/lib/python3.12/dist-packages (24.1.2)\n",
      "Collecting pip\n",
      "  Downloading pip-25.3-py3-none-any.whl.metadata (4.7 kB)\n",
      "Downloading pip-25.3-py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 24.1.2\n",
      "    Uninstalling pip-24.1.2:\n",
      "      Successfully uninstalled pip-24.1.2\n",
      "Successfully installed pip-25.3\n"
     ]
    }
   ],
   "source": [
    "# --- Check Python and pip versions ---\n",
    "!python --version\n",
    "!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30a6995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
      "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.61.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n"
     ]
    }
   ],
   "source": [
    "# --- Install required libraries ---\n",
    "!pip install torch\n",
    "!pip install numpy pandas scikit-learn matplotlib seaborn\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f355596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Setup & Imports ---\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from collections import Counter, defaultdict\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torch.cuda.amp import GradScaler\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.cuda import is_available\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForTokenClassification, AutoConfig,\n",
    "    get_scheduler, DataCollatorForTokenClassification\n",
    ")\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    classification_report, accuracy_score, f1_score,\n",
    "    precision_score, recall_score\n",
    ")\n",
    "from itertools import chain\n",
    "from copy import deepcopy\n",
    "\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b792b97a",
   "metadata": {},
   "source": [
    "### Colab Pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2dda504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Dec 17 15:55:27 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   45C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# --- Check GPU availability ---\n",
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Not connected to a GPU')\n",
    "else:\n",
    "  print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3645ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your runtime has 13.6 gigabytes of available RAM\n",
      "\n",
      "Not using a high-RAM runtime\n"
     ]
    }
   ],
   "source": [
    "# --- Check RAM availability ---\n",
    "from psutil import virtual_memory\n",
    "ram_gb = virtual_memory().total / 1e9\n",
    "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
    "\n",
    "if ram_gb < 20:\n",
    "  print('Not using a high-RAM runtime')\n",
    "else:\n",
    "  print('You are using a high-RAM runtime!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7982656a",
   "metadata": {},
   "source": [
    "### Paths setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fa3083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# --- Mount Google Drive (for Google Colab users) ---\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ff2316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project path: /content/drive/MyDrive/Projects/Laboratory4/\n",
      "Data path: /content/drive/MyDrive/Projects/Laboratory4/data/\n",
      "Results path: /content/drive/MyDrive/Projects/Laboratory4/results/\n"
     ]
    }
   ],
   "source": [
    "# --- Define Paths ---\n",
    "laboratory = 'Laboratory4'\n",
    "\n",
    "base_path = '/content/drive/MyDrive/'\n",
    "project_path = base_path + f'Projects/{laboratory}/'\n",
    "data_path = project_path + 'data/'\n",
    "results_path = project_path + 'results/'\n",
    "\n",
    "# Ensure directories exist\n",
    "os.makedirs(project_path, exist_ok=True)\n",
    "os.makedirs(data_path, exist_ok=True)\n",
    "os.makedirs(results_path, exist_ok=True)\n",
    "\n",
    "print(f\"Project path: {project_path}\")\n",
    "print(f\"Data path: {data_path}\")\n",
    "print(f\"Results path: {results_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c716a9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00e06e3",
   "metadata": {},
   "source": [
    "## Helper Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885efde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# GLOBAL CONFIGURATION: Plot Saving (Colab/Google Drive only)\n",
    "# ============================================================================\n",
    "SAVE_PLOTS = 1\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "BASE_DIR = results_path + 'Task4'\n",
    "os.makedirs(BASE_DIR, exist_ok=True)\n",
    "\n",
    "def save_figure_for_report(filename, dpi=300, bbox_inches='tight'):\n",
    "    \"\"\"\n",
    "    Save the current matplotlib figure for use in the report.\n",
    "\n",
    "    Args:\n",
    "        filename: Name of the file (e.g., 'class_distribution.png')\n",
    "        dpi: Resolution (default 300 for high quality)\n",
    "        bbox_inches: Bounding box setting (default 'tight' to remove whitespace)\n",
    "    \"\"\"\n",
    "    if not SAVE_PLOTS:\n",
    "        return  # Skip saving if flag is disabled or filename missing\n",
    "\n",
    "    filepath = os.path.join(BASE_DIR, filename)\n",
    "    plt.savefig(filepath, dpi=dpi, bbox_inches=bbox_inches)\n",
    "    print(f\"Figure saved to: {filepath}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a0ddf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(full_predictions, full_labels):\n",
    "    \"\"\"Compute token-level classification metrics\"\"\"\n",
    "    flat_predictions = list(chain(*full_predictions))\n",
    "    flat_labels = list(chain(*full_labels))\n",
    "\n",
    "    token_accuracy = accuracy_score(flat_labels, flat_predictions)\n",
    "    token_precision = precision_score(flat_labels, flat_predictions, average='macro', zero_division=0)\n",
    "    token_recall = recall_score(flat_labels, flat_predictions, average='macro', zero_division=0)\n",
    "    token_f1 = f1_score(flat_labels, flat_predictions, average='macro', zero_division=0)\n",
    "\n",
    "    metrics = {\n",
    "        \"token_accuracy\": token_accuracy,\n",
    "        \"token_precision\": token_precision,\n",
    "        \"token_recall\": token_recall,\n",
    "        \"token_f1\": token_f1,\n",
    "    }\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe309ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(predictions, labels):\n",
    "    \"\"\"Convert predictions and labels to original label format\"\"\"\n",
    "    predictions = predictions.detach().cpu().clone().numpy()\n",
    "    labels = labels.detach().cpu().clone().numpy()\n",
    "    true_labels = [[id2label[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [id2label[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    return true_predictions, true_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfaef64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    \"\"\"Align word-level labels to token-level labels\"\"\"\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id != current_word:\n",
    "            current_word = word_id\n",
    "            label = -100 if word_id is None else labels[word_id]\n",
    "            new_labels.append(label)\n",
    "        elif word_id is None:\n",
    "            new_labels.append(-100)\n",
    "        else:\n",
    "            label = labels[word_id]\n",
    "            new_labels.append(label)\n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7aeed2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels_unixcoder(samples, tokenizer):\n",
    "    \"\"\"Tokenize and align labels for UniXcoder\"\"\"\n",
    "    split_sentences = [s.split(\" \") for s in samples[\"session\"]]\n",
    "\n",
    "    tokenized = tokenizer(\n",
    "        split_sentences,\n",
    "        truncation=True,\n",
    "        is_split_into_words=True\n",
    "    )\n",
    "\n",
    "    all_labels = samples[\"label_id\"]\n",
    "    aligned_all = []\n",
    "\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized.word_ids(i)\n",
    "\n",
    "        aligned = []\n",
    "        prev_word = None\n",
    "\n",
    "        for wid in word_ids:\n",
    "            if wid is None:\n",
    "                aligned.append(-100)\n",
    "            else:\n",
    "                if wid != prev_word:\n",
    "                    aligned.append(labels[wid])\n",
    "                    prev_word = wid\n",
    "                else:\n",
    "                    aligned.append(-100)\n",
    "        aligned_all.append(aligned)\n",
    "\n",
    "    tokenized[\"labels\"] = aligned_all\n",
    "    return tokenized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99eae25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(model, optimizer, lr_scheduler, train_loader, val_loader, device, num_epochs):\n",
    "    scaler = torch.amp.GradScaler()\n",
    "    best_val_loss = np.inf\n",
    "    best_weights = deepcopy(model.state_dict())\n",
    "\n",
    "    # Calculate steps dynamically based on the passed loader\n",
    "    num_training_steps = num_epochs * len(train_loader)\n",
    "    progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "    train_losses, val_losses = [], []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch in train_loader:\n",
    "            # Move batch to device\n",
    "            batch = {k: v.to(device, non_blocking=True) for k, v in batch.items()}\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            # Automatic Mixed Precision\n",
    "            with torch.amp.autocast(device_type='cuda'):\n",
    "                outputs = model(**batch)\n",
    "                loss = outputs.loss\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            lr_scheduler.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            progress_bar.update(1)\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        predictions_list, labels_list = [], []\n",
    "\n",
    "        for batch in val_loader:\n",
    "            batch = {key: value.to(device, non_blocking=True) for key, value in batch.items()}\n",
    "            with torch.no_grad():\n",
    "                with torch.amp.autocast(device_type='cuda'):  # ← FIXED HERE\n",
    "                    outputs = model(**batch)\n",
    "\n",
    "            val_loss += outputs.loss.item()\n",
    "            predictions = outputs.logits.argmax(dim=-1)\n",
    "            labels = batch[\"labels\"]\n",
    "\n",
    "            true_predictions, true_labels = postprocess(predictions, labels)\n",
    "            predictions_list += true_predictions\n",
    "            labels_list += true_labels\n",
    "\n",
    "        # Compute validation metrics\n",
    "        val_metrics = compute_metrics(predictions_list, labels_list)\n",
    "        val_accuracy = val_metrics[\"token_accuracy\"]\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "        # Optional: Print progress\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        if avg_val_loss <= best_val_loss:\n",
    "            best_epoch = epoch\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_weights = deepcopy(model.state_dict())\n",
    "\n",
    "    # Load the best weights found during this specific training run\n",
    "    model.load_state_dict(best_weights)\n",
    "    return model, best_epoch, best_val_loss, train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2907b27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, device):\n",
    "    \"\"\"Evaluate model on a dataset\"\"\"\n",
    "    model.eval()\n",
    "    full_predictions, full_labels = [], []\n",
    "    for batch in dataloader:\n",
    "        batch = {key: value.to(device) for key, value in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "        predictions = outputs.logits.argmax(dim=-1)\n",
    "        labels = batch[\"labels\"]\n",
    "        true_predictions, true_labels = postprocess(predictions, labels)\n",
    "        full_predictions += true_predictions\n",
    "        full_labels += true_labels\n",
    "\n",
    "    test_metrics = compute_metrics(full_predictions, full_labels)\n",
    "    return full_predictions, full_labels, test_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68532a13",
   "metadata": {},
   "source": [
    "### Re-Train Best Model from Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd359ef6",
   "metadata": {},
   "source": [
    "### Dataset Loading and preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89509fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training and test data\n",
    "train_df = pd.read_json(f\"{data_path}train.json\")\n",
    "test_df = pd.read_json(f\"{data_path}test.json\")\n",
    "\n",
    "print(f\"Training dataset: {train_df.shape[0]} sessions\")\n",
    "print(f\"Test dataset: {test_df.shape[0]} sessions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c868680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training data into train and validation\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)\n",
    "print(f\"Split training: {train_df.shape[0]} train, {val_df.shape[0]} validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01736845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create label mappings\n",
    "unique_labels = list(train_df.label.explode().unique())\n",
    "print(f\"Unique labels: {unique_labels}\")\n",
    "\n",
    "id2label = {it: label for it, label in enumerate(unique_labels)}\n",
    "label2id = {label: it for it, label in enumerate(unique_labels)}\n",
    "\n",
    "print(f\"Label mappings: {label2id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fc3a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Hugging Face datasets\n",
    "full_ds = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(train_df.reset_index(drop=True)),\n",
    "    \"valid\": Dataset.from_pandas(val_df.reset_index(drop=True)),\n",
    "    \"test\": Dataset.from_pandas(test_df.reset_index(drop=True)),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e751c93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to IDs\n",
    "def convert_labels_to_ids(sample):\n",
    "    sample['label_id'] = [label2id[el] for el in sample[\"label\"]]\n",
    "    return sample\n",
    "\n",
    "encoded_dataset = full_ds.map(convert_labels_to_ids)\n",
    "print(f\"Encoded dataset: {encoded_dataset}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffa05cc",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02b8f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tokenizer and model checkpoint\n",
    "unixcoder_model_checkpoint = \"microsoft/unixcoder-base\"\n",
    "unixcoder_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    unixcoder_model_checkpoint,\n",
    "    add_prefix_space=True,\n",
    "    use_fast=True,\n",
    "    model_max_length=512\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b09e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize datasets\n",
    "original_columns = encoded_dataset[\"train\"].column_names\n",
    "tokenized_datasets = encoded_dataset.map(\n",
    "    lambda x: tokenize_and_align_labels_unixcoder(x, unixcoder_tokenizer),\n",
    "    batched=True,\n",
    "    remove_columns=original_columns,\n",
    ")\n",
    "\n",
    "print(f\"Tokenized dataset: {tokenized_datasets}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4326502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data collator\n",
    "data_collator = DataCollatorForTokenClassification(\n",
    "    tokenizer=unixcoder_tokenizer,\n",
    "    return_tensors=\"pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0435d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = 2\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"train\"],\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"valid\"],\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"test\"],\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(\"DataLoaders created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecc588a",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9966fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SECTION 3: TRAIN BEST MODEL FROM TASK 3\")\n",
    "print(\"UNIXCODER FULL FINE-TUNE WITH LR=1e-05\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "N_TRAIN_EPOCHS = 40\n",
    "BEST_LR = 1e-5\n",
    "\n",
    "print(f\"\\nTraining parameters:\")\n",
    "print(f\"  - Model: UniXcoder (microsoft/unixcoder-base)\")\n",
    "print(f\"  - Learning Rate: {BEST_LR}\")\n",
    "print(f\"  - Epochs: {N_TRAIN_EPOCHS}\")\n",
    "print(f\"  - Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"  - Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb52ab57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "best_model = AutoModelForTokenClassification.from_pretrained(\n",
    "    unixcoder_model_checkpoint,\n",
    "    num_labels=len(id2label),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0d9364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup optimizer and scheduler\n",
    "optimizer = optim.AdamW(best_model.parameters(), lr=BEST_LR)\n",
    "\n",
    "num_training_steps = N_TRAIN_EPOCHS * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b5ba94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "best_model, best_epoch, best_val_loss, train_losses, val_losses = training_loop(\n",
    "    best_model, optimizer, lr_scheduler, \n",
    "    train_dataloader, eval_dataloader, device, N_TRAIN_EPOCHS\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"Training complete! Best epoch: {best_epoch + 1}, Best val loss: {best_val_loss:.4f}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e7a7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "print(\"\\nEvaluating on test set...\")\n",
    "test_preds, test_labels, test_metrics = evaluate_model(best_model, test_dataloader, device)\n",
    "\n",
    "print(f\"\\nTest Set Performance:\")\n",
    "print(f\"  - Token Accuracy: {test_metrics['token_accuracy']:.4f}\")\n",
    "print(f\"  - Macro F1-Score: {test_metrics['token_f1']:.4f}\")\n",
    "print(f\"  - Macro Precision: {test_metrics['token_precision']:.4f}\")\n",
    "print(f\"  - Macro Recall: {test_metrics['token_recall']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47db163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best model for later use in inference\n",
    "model_save_path = os.path.join(results_path, \"best_unixcoder_model_task3\")\n",
    "os.makedirs(model_save_path, exist_ok=True)\n",
    "best_model.save_pretrained(model_save_path)\n",
    "unixcoder_tokenizer.save_pretrained(model_save_path)\n",
    "\n",
    "print(f\"\\nBest model saved to: {model_save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c0f099",
   "metadata": {},
   "source": [
    "## Inference Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09daf6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cyberlab.csv - the actual inference dataset\n",
    "cyberlab_df = pd.read_csv(f\"{data_path}cyberlab.csv\")\n",
    "\n",
    "print(f\"Cyberlab dataset: {len(cyberlab_df)} sessions\")\n",
    "print(f\"Columns: {list(cyberlab_df.columns)}\")\n",
    "print(f\"Data types:\\n{cyberlab_df.dtypes}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(cyberlab_df.head(3))\n",
    "\n",
    "# Convert timestamps_statements to datetime\n",
    "cyberlab_df['timestamps_statements'] = pd.to_datetime(cyberlab_df['timestamps_statements'])\n",
    "cyberlab_df['date'] = cyberlab_df['timestamps_statements'].dt.date\n",
    "\n",
    "print(f\"\\nDate range: {cyberlab_df['date'].min()} to {cyberlab_df['date'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8506d598",
   "metadata": {},
   "source": [
    "## Inference Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f4b3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_long_words(session, max_length=20):\n",
    "    \"\"\"Truncate words longer than max_length (matching Task 3 preprocessing)\"\"\"\n",
    "    words = session.split()\n",
    "    truncated = []\n",
    "    for word in words:\n",
    "        if len(word) > max_length:\n",
    "            truncated.append(word[:max_length-3] + '...')\n",
    "        else:\n",
    "            truncated.append(word)\n",
    "    return ' '.join(truncated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32f96d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_predictions_to_words(predictions_ids, word_ids, id2label):\n",
    "    \"\"\"\n",
    "    Extract only the prediction for the first token of each word.\n",
    "    Returns list of predicted tactics, one per word.\n",
    "    \"\"\"\n",
    "    aligned_preds_ids = []\n",
    "    seen_words = set()\n",
    "    \n",
    "    for idx, word_id in enumerate(word_ids):\n",
    "        if word_id is None:\n",
    "            continue\n",
    "        if word_id not in seen_words:\n",
    "            aligned_preds_ids.append(predictions_ids[idx])\n",
    "            seen_words.add(word_id)\n",
    "    \n",
    "    aligned_preds = [id2label[pred_id] for pred_id in aligned_preds_ids]\n",
    "    return aligned_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7be1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tactics(session_text, model, tokenizer, device, id2label):\n",
    "    \"\"\"\n",
    "    Predict MITRE tactics for a session.\n",
    "    Returns:\n",
    "    - words: list of words\n",
    "    - predictions: list of predicted tactics (one per word)\n",
    "    \"\"\"\n",
    "    # Preprocess: truncate long words\n",
    "    session_clean = truncate_long_words(session_text)\n",
    "    words = session_clean.split()\n",
    "    \n",
    "    # Tokenize\n",
    "    tokenized = tokenizer(\n",
    "        words,\n",
    "        truncation=True,\n",
    "        is_split_into_words=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    # Move to device\n",
    "    tokenized = {k: v.to(device) for k, v in tokenized.items()}\n",
    "    \n",
    "    # Inference\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokenized)\n",
    "        logits = outputs.logits\n",
    "    \n",
    "    # Get predictions\n",
    "    predictions_ids = logits.argmax(dim=-1).cpu().numpy()[0]\n",
    "    word_ids = tokenized.word_ids(0)\n",
    "    \n",
    "    # Align to words (first token per word)\n",
    "    aligned_preds = align_predictions_to_words(predictions_ids, word_ids, id2label)\n",
    "    \n",
    "    return words, aligned_preds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335dd2df",
   "metadata": {},
   "source": [
    "## Batch Inference on Cyberlab Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b894a566",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "errors = []\n",
    "\n",
    "print(f\"Processing {len(cyberlab_df)} sessions from cyberlab...\\n\")\n",
    "\n",
    "for idx, row in cyberlab_df.iterrows():\n",
    "    try:\n",
    "        session_text = row['session']\n",
    "        \n",
    "        # Predict tactics\n",
    "        words, predictions = predict_tactics(session_text, best_model, unixcoder_tokenizer, device, id2label)\n",
    "        \n",
    "        # Store result with temporal information\n",
    "        results.append({\n",
    "            'session_id': idx,\n",
    "            'session_text': session_text,\n",
    "            'words': words,\n",
    "            'predictions': predictions,\n",
    "            'fingerprint': tuple(predictions),\n",
    "            'timestamp': row['timestamps_statements'],\n",
    "            'date': row['date'],\n",
    "            'country': row['country_name']\n",
    "        })\n",
    "        \n",
    "        if (idx + 1) % 50 == 0:\n",
    "            print(f\"  Processed {idx + 1}/{len(cyberlab_df)} sessions...\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        errors.append({'session_id': idx, 'error': str(e)})\n",
    "        print(f\"  ERROR on session {idx}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed848882",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "print(f\"\\n✓ Successfully processed: {len(results_df)}/{len(cyberlab_df)} sessions\")\n",
    "if errors:\n",
    "    print(f\"✗ Errors: {len(errors)}\")\n",
    "\n",
    "print(f\"\\nResults dataframe shape: {results_df.shape}\")\n",
    "print(results_df.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd336e16",
   "metadata": {},
   "source": [
    "## Command Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7862ecdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "commands_of_interest = ['cat', 'grep', 'echo', 'rm']\n",
    "command_analysis = defaultdict(lambda: {'tags': [], 'examples': defaultdict(list)})\n",
    "\n",
    "for _, row in results_df.iterrows():\n",
    "    words = row['words']\n",
    "    predictions = row['predictions']\n",
    "    session_text = row['session_text']\n",
    "    \n",
    "    # Only consider words that have predictions (handle truncation)\n",
    "    for word_idx in range(min(len(words), len(predictions))):\n",
    "        word = words[word_idx]\n",
    "        \n",
    "        if word in commands_of_interest:\n",
    "            tag = predictions[word_idx]\n",
    "            command_analysis[word]['tags'].append(tag)\n",
    "            \n",
    "            # Store example per (command, tag) pair - keep up to 2 examples\n",
    "            if len(command_analysis[word]['examples'][tag]) < 2:\n",
    "                command_analysis[word]['examples'][tag].append({\n",
    "                    'session': session_text[:300],\n",
    "                    'words': words[:20],\n",
    "                    'predictions': predictions[:20]\n",
    "                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1510878",
   "metadata": {},
   "outputs": [],
   "source": [
    "command_tag_table = []\n",
    "for cmd in commands_of_interest:\n",
    "    if cmd in command_analysis:\n",
    "        tags = command_analysis[cmd]['tags']\n",
    "        if len(tags) > 0:\n",
    "            tag_counts = Counter(tags)\n",
    "            total = len(tags)\n",
    "            \n",
    "            for tag, count in tag_counts.most_common():\n",
    "                freq = (count / total) * 100\n",
    "                command_tag_table.append({\n",
    "                    'Command': cmd,\n",
    "                    'Tag': tag,\n",
    "                    'Count': count,\n",
    "                    'Frequency (%)': f\"{freq:.1f}\"\n",
    "                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9abcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if command_tag_table:\n",
    "    command_tag_df = pd.DataFrame(command_tag_table)\n",
    "    print(command_tag_df.to_string(index=False))\n",
    "    command_tag_df.to_csv(f\"{results_path}command_tag_frequency.csv\", index=False)\n",
    "    \n",
    "    # Determine if commands are uniquely associated with single tag\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"Command Uniqueness Analysis\")\n",
    "    print(\"-\"*80)\n",
    "    for cmd in commands_of_interest:\n",
    "        if cmd in command_analysis:\n",
    "            unique_tags = len(set(command_analysis[cmd]['tags']))\n",
    "            total_occurrences = len(command_analysis[cmd]['tags'])\n",
    "            print(f\"'{cmd}': {unique_tags} unique tag(s) across {total_occurrences} occurrences\", end=\"\")\n",
    "            if unique_tags == 1:\n",
    "                print(\" ✓ (uniquely associated)\")\n",
    "            else:\n",
    "                print(\" ✗ (NOT uniquely associated)\")\n",
    "    \n",
    "    # Plot command-tag distribution\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('MITRE Tag Distribution by Command', fontsize=18, fontweight='bold', y=1.00)\n",
    "    \n",
    "    for idx, cmd in enumerate(commands_of_interest):\n",
    "        ax = axes[idx // 2, idx % 2]\n",
    "        \n",
    "        if cmd in command_analysis and len(command_analysis[cmd]['tags']) > 0:\n",
    "            tags = command_analysis[cmd]['tags']\n",
    "            tag_counts = Counter(tags)\n",
    "            \n",
    "            tags_list = list(tag_counts.keys())\n",
    "            counts = list(tag_counts.values())\n",
    "            \n",
    "            colors = sns.color_palette(\"husl\", len(tags_list))\n",
    "            bars = ax.bar(tags_list, counts, color=colors, edgecolor='black', linewidth=1.5)\n",
    "            \n",
    "            # Add count labels on bars\n",
    "            for bar in bars:\n",
    "                height = bar.get_height()\n",
    "                ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                       f'{int(height)}',\n",
    "                       ha='center', va='bottom', fontweight='bold')\n",
    "            \n",
    "            ax.set_title(f\"Command: '{cmd}' (n={len(tags)})\", fontsize=14, fontweight='bold')\n",
    "            ax.set_ylabel(\"Frequency\", fontsize=12, fontweight='bold')\n",
    "            ax.set_xlabel(\"MITRE Tactic\", fontsize=12, fontweight='bold')\n",
    "            ax.tick_params(axis='x', rotation=45, labelsize=11)\n",
    "            ax.tick_params(axis='y', labelsize=11)\n",
    "            ax.grid(axis='y', alpha=0.3)\n",
    "        else:\n",
    "            ax.text(0.5, 0.5, f\"No '{cmd}' found\", ha='center', va='center', fontsize=14)\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{results_path}command_tag_distribution.png\", dpi=300, bbox_inches='tight')\n",
    "    print(f\"\\n✓ Saved: command_tag_distribution.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Print detailed examples for each (command, tag) pair\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"COMMAND-TAG EXAMPLES\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for cmd in commands_of_interest:\n",
    "        if cmd in command_analysis and command_analysis[cmd]['examples']:\n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"COMMAND: '{cmd.upper()}'\")\n",
    "            print(f\"{'='*80}\")\n",
    "            \n",
    "            for tag in sorted(command_analysis[cmd]['examples'].keys()):\n",
    "                examples = command_analysis[cmd]['examples'][tag]\n",
    "                print(f\"\\n  Tag: {tag}\")\n",
    "                print(f\"  Number of examples: {len(examples)}\")\n",
    "                \n",
    "                if examples:\n",
    "                    ex = examples[0]\n",
    "                    print(f\"\\n  Example session:\")\n",
    "                    print(f\"    Words (first 20): {ex['words']}\")\n",
    "                    print(f\"    Predictions: {ex['predictions']}\")\n",
    "                    print(f\"    Session (truncated): {ex['session'][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ea54e2",
   "metadata": {},
   "source": [
    "## FingerPrint Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01922c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find unique fingerprints and sort by first appearance date\n",
    "unique_fingerprints = results_df['fingerprint'].unique()\n",
    "print(f\"\\nTotal unique fingerprints: {len(unique_fingerprints)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f9fa21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create fingerprint table with metadata\n",
    "fingerprint_data = []\n",
    "for fp in unique_fingerprints:\n",
    "    fp_sessions = results_df[results_df['fingerprint'] == fp]\n",
    "    first_date = fp_sessions['date'].min()\n",
    "    last_date = fp_sessions['date'].max()\n",
    "    session_count = len(fp_sessions)\n",
    "    days_active = (fp_sessions['date'].nunique())\n",
    "    \n",
    "    fingerprint_data.append({\n",
    "        'fingerprint': fp,\n",
    "        'first_seen': first_date,\n",
    "        'last_seen': last_date,\n",
    "        'session_count': session_count,\n",
    "        'days_active': days_active,\n",
    "        'fingerprint_length': len(fp)\n",
    "    })\n",
    "\n",
    "fingerprint_df = pd.DataFrame(fingerprint_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278e4fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by first appearance date\n",
    "fingerprint_df = fingerprint_df.sort_values('first_seen').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24115db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign fingerprint IDs\n",
    "fingerprint_df['fp_id'] = range(len(fingerprint_df))\n",
    "\n",
    "print(f\"\\nTop 15 Fingerprints by Session Count:\")\n",
    "print(\"-\"*80)\n",
    "top_fps = fingerprint_df.nlargest(15, 'session_count')\n",
    "print(top_fps[['fp_id', 'first_seen', 'session_count', 'days_active', 'fingerprint_length']].to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9408fb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge fingerprint IDs back to results\n",
    "fp_to_id = dict(zip(fingerprint_df['fingerprint'], fingerprint_df['fp_id']))\n",
    "results_df['fp_id'] = results_df['fingerprint'].map(fp_to_id)\n",
    "\n",
    "# Count sessions per fingerprint per day\n",
    "daily_fp_counts = results_df.groupby(['date', 'fp_id']).size().reset_index(name='session_count')\n",
    "\n",
    "print(f\"\\nDaily fingerprint counts: {len(daily_fp_counts)} date-fingerprint pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7dc9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save fingerprint data\n",
    "fingerprint_df.to_csv(f\"{results_path}fingerprints.csv\", index=False)\n",
    "daily_fp_counts.to_csv(f\"{results_path}daily_fingerprint_counts.csv\", index=False)\n",
    "print(f\"✓ Saved: fingerprints.csv and daily_fingerprint_counts.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fea1b2f",
   "metadata": {},
   "source": [
    "## Fingerprint Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd732e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(18, 12))\n",
    "\n",
    "# Prepare data for scatter plot\n",
    "dates = daily_fp_counts['date']\n",
    "fp_ids = daily_fp_counts['fp_id']\n",
    "sizes = daily_fp_counts['session_count'] * 40  # Scale point size\n",
    "colors = daily_fp_counts['session_count']  # Color by session count\n",
    "\n",
    "# Create scatter plot\n",
    "scatter = ax.scatter(\n",
    "    dates, fp_ids,\n",
    "    s=sizes,\n",
    "    c=colors,\n",
    "    cmap='YlOrRd',\n",
    "    alpha=0.7,\n",
    "    edgecolors='black',\n",
    "    linewidth=0.8\n",
    ")\n",
    "\n",
    "ax.set_xlabel('Date', fontsize=16, fontweight='bold')\n",
    "ax.set_ylabel('Fingerprint ID (sorted by first appearance)', fontsize=16, fontweight='bold')\n",
    "ax.set_title('MITRE Tactics Fingerprints Over Time in Honeypot', fontsize=18, fontweight='bold', pad=20)\n",
    "ax.grid(True, alpha=0.3, linestyle='--')\n",
    "\n",
    "# Add colorbar\n",
    "cbar = plt.colorbar(scatter, ax=ax)\n",
    "cbar.set_label('Number of Sessions per Day', fontsize=14, fontweight='bold')\n",
    "cbar"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
