\section{Model training}\label{sec:task3}
In this section, we train and compare different transformer-based models for token-level classification of SSH sessions into MITRE tactics. We evaluate: (1)~pre-trained BERT, (2)~randomly initialized (naked) BERT, (3)~pre-trained UniXcoder, and (4)~alternative fine-tuning strategies with frozen layers. For each configuration, we perform a grid search over learning rates and report the best results. We split the data into 80\% training and 20\% validation, using the provided test set for final evaluation.

\subsection{Fine-tuning Pre-trained BERT}
We fine-tune a pre-trained BERT model (google-bert/bert-base-uncased) for token classification. After grid search over learning rates ($5\times10^{-6}$, $10^{-5}$, $2\times10^{-5}$), we select $lr=10^{-5}$ with 20 epochs based on validation macro F1-score and loss stability. \Cref{fig:task3_bert_curves} shows the learning curves for different learning rates, and \Cref{fig:task3_bert_f1} reports the per-class F1-scores on the test set.

The model achieves strong performance with only 251 training samples: \SI{86.94}{\percent} token accuracy, \SI{63.99}{\percent} macro F1-score, and \SI{82.67}{\percent} session fidelity on the test set. The main difficulties emerge in minority classes with few training examples and in distinguishing semantically similar tactics. The recall-precision imbalance (0.57 vs 0.84) indicates that the model is conservative, avoiding false positives but missing some minority class examples.

\begin{figure}
	\centering
    \subfloat[][{Training loss.}\label{fig:task3_bert_train}]
	{\includegraphics[width=.48\linewidth]{img/Task3/task3_bert_fine_tuned_train_loss.png}} \quad
	\subfloat[][{Validation loss.}\label{fig:task3_bert_val}]
	{\includegraphics[width=.48\linewidth]{img/Task3/task3_bert_fine_tuned_val_loss.png}} \\
  	\caption{Training and validation loss curves for pre-trained BERT with different learning rates.}\label{fig:task3_bert_curves}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[width=0.75\linewidth]{img/Task3/per_class_f1_bert_fine-tuned_1e-05_test.png}
	\caption{Per-class F1-scores on the test set for pre-trained BERT ($lr=10^{-5}$, 20 epochs). The bar labels include the support count for each class.}
	\label{fig:task3_bert_f1}
\end{figure}

\subsection{Naked BERT Baseline}
To verify that pre-training is essential, we train a naked BERT model---i.e., the same architecture initialized with random weights instead of pre-trained ones---from scratch. After grid search ($5\times10^{-6}$, $10^{-5}$, $5\times10^{-5}$), we select $lr=5\times10^{-5}$ with 25 epochs.

\Cref{tab:task3_bert_comparison} compares the results. The naked BERT achieves only \SI{74.02}{\percent} token accuracy, a significant 12 percentage point drop compared to the pre-trained version. This confirms that SSH command classification is not a simple problem: the 251 training samples are insufficient for learning from scratch, and pre-trained weights provide essential linguistic knowledge that the naked model cannot rediscover.

\begin{table}
\small
\centering
\caption{Comparison between pre-trained BERT and naked BERT on the test set.}
\label{tab:task3_bert_comparison}
\begin{tabular}{lccccc}
\toprule
\textbf{Model} & \textbf{Token Acc.} & \textbf{Macro Prec.} & \textbf{Macro Rec.} & \textbf{Macro F1} & \textbf{Fidelity} \\
\midrule
Pre-trained BERT  & 0.8694 & 0.8402 & 0.5746 & 0.6399 & 0.8267 \\
Naked BERT        & 0.7402 & 0.5469 & 0.4775 & 0.4925 & 0.7701 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Fine-tuning UniXcoder}
Since UniXcoder was pre-trained on a large code corpus, we hypothesize it has more relevant prior knowledge for SSH command analysis. After grid search ($5\times10^{-6}$, $10^{-5}$, $4\times10^{-5}$), we select $lr=10^{-5}$ with 15 epochs. \Cref{fig:task3_unixcoder_curves} shows the learning curves.

The hypothesis is confirmed: UniXcoder achieves \SI{89.01}{\percent} token accuracy, outperforming pre-trained BERT by 2.07 percentage points. The macro F1-score improves by \SI{5.10}{\percent} (from 0.6399 to 0.6909). UniXcoder's specialized vocabulary and representations learned from code corpora enable better handling of shell syntax, operators (pipes, redirects), and path structures. UniXcoder is particularly better at identifying \emph{Defense Evasion} samples (F1-score 0.758 vs 0.404 for BERT).

\begin{figure}
	\centering
    \subfloat[][{Training loss.}\label{fig:task3_unix_train}]
	{\includegraphics[width=.48\linewidth]{img/Task3/task3_unixcoder_fine_tuned_train_loss.png}} \quad
	\subfloat[][{Validation loss.}\label{fig:task3_unix_val}]
	{\includegraphics[width=.48\linewidth]{img/Task3/task3_unixcoder_fine_tuned_val_loss.png}} \\
  	\caption{Training and validation loss curves for UniXcoder with different learning rates.}\label{fig:task3_unixcoder_curves}
\end{figure}

\subsection{Alternative Fine-tuning Strategies}
We explore parameter-efficient fine-tuning by freezing portions of the UniXcoder encoder. We test two configurations: (1)~training only the last 2 encoder layers + classification head, and (2)~training only the classification head. \Cref{tab:task3_alternative} summarizes the results.

\begin{table}
\small
\centering
\caption{Parameter-efficient fine-tuning comparison for UniXcoder on the test set.}
\label{tab:task3_alternative}
\begin{tabular}{lcccccc}
\toprule
\textbf{Configuration} & \textbf{Best LR} & \textbf{Trainable \%} & \textbf{Token Acc.} & \textbf{Macro F1} & \textbf{Fidelity} \\
\midrule
Full Fine-Tune     & $10^{-5}$       & 100.00\%  & 0.8901 & 0.6909 & 0.8411 \\
Last 2 + Head      & $5\times10^{-5}$ & 11.31\%   & 0.8714 & 0.6521 & 0.8294 \\
Head Only          & $10^{-3}$        & 0.004\%   & 0.8035 & 0.5628 & 0.7527 \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Parameter counts} Full fine-tuning trains all 125.3M parameters. The ``Last 2 + Head'' configuration trains 14.18M parameters (\SI{11.31}{\percent}), while ``Head Only'' trains merely 5.38k parameters (\SI{0.004}{\percent}).

\paragraph{Training speed} Training speed improves significantly with fewer trainable parameters: full fine-tuning requires $\approx$\SI{5}{\second\per\epoch}, ``Last 2 + Head'' achieves \SI{2}{\second\per\epoch}, and ``Head Only'' reaches \SI{1.6}{\second\per\epoch} (3-4$\times$ speedup).

\paragraph{Learning rate requirements} Different learning rates are required: full fine-tune and ``Last 2 + Head'' use $lr\approx10^{-5}$, while ``Head Only'' requires a much higher $lr=10^{-3}$ because the frozen encoder provides stable features and the tiny classifier can tolerate aggressive updates.

\paragraph{Performance trade-offs} The ``Last 2 + Head'' configuration loses only \SI{1.87}{\percent} accuracy while reducing trainable parameters by \SI{88.69}{\percent}, representing a good efficiency-performance trade-off. However, ``Head Only'' loses \SI{8.66}{\percent} accuracy because the frozen encoder cannot adapt its representations to the specialized SSH domain. \Cref{fig:task3_alternative_f1} shows the per-class F1-scores for all UniXcoder configurations.

\begin{figure}
	\centering
    \subfloat[][{Full Fine-Tune.}\label{fig:task3_unix_full}]
	{\includegraphics[width=.48\linewidth]{img/Task3/per_class_f1_unixcoder_fine-tuned_1e-05_test.png}} \quad
	\subfloat[][{Last 2 + Head.}\label{fig:task3_unix_l2h}]
	{\includegraphics[width=.48\linewidth]{img/Task3/per_class_f1_unixcoder_last_2_+_head_5e-05_test.png}} \\
	\subfloat[][{Head Only.}\label{fig:task3_unix_head}]
	{\includegraphics[width=.48\linewidth]{img/Task3/per_class_f1_unixcoder_head_only_0.001_test.png}}
  	\caption{Per-class F1-scores on the test set for different UniXcoder fine-tuning strategies.}\label{fig:task3_alternative_f1}
\end{figure}

\subsection{Summary and Best Model Selection}
\Cref{tab:task3_summary} summarizes all models. The fully fine-tuned UniXcoder achieves the best performance with \SI{89.01}{\percent} token accuracy and \SI{69.09}{\percent} macro F1-score. We select this model ($lr=10^{-5}$, 15 epochs) for the inference task in \cref{sec:task3}.

\begin{table}
\small
\centering
\caption{Summary of all models on the test set. The best results are highlighted in bold.}
\label{tab:task3_summary}
\begin{tabular}{lccccc}
\toprule
\textbf{Model} & \textbf{Token Acc.} & \textbf{Macro Prec.} & \textbf{Macro Rec.} & \textbf{Macro F1} & \textbf{Fidelity} \\
\midrule
Pre-trained BERT      & 0.8694 & 0.8402          & 0.5746 & 0.6399 & 0.8267 \\
Naked BERT            & 0.7402 & 0.5469          & 0.4775 & 0.4925 & 0.7701 \\
UniXcoder (Full)      & \textbf{0.8901} & 0.8359 & \textbf{0.6265} & \textbf{0.6909} & \textbf{0.8411} \\
UniXcoder (Last 2+H)  & 0.8714 & 0.8043          & 0.5972 & 0.6521 & 0.8294 \\
UniXcoder (Head Only) & 0.8035 & 0.7206          & 0.5093 & 0.5628 & 0.7527 \\
\bottomrule
\end{tabular}
\end{table}