\section{Model training}\label{sec:task3}
In this section, we train and compare different transformer-based models for token-level classification of SSH sessions into MITRE tactics. We evaluate: (1)~pre-trained BERT, (2)~randomly initialized (naked) BERT, (3)~pre-trained UniXcoder, and (4)~alternative fine-tuning strategies with frozen layers. For each configuration, we perform a grid search over learning rates and select the best model based on the validation loss and macro F1-score. Specifically, the validation loss allows detecting divergence or instability typical of overfitting, while validation macro
F1-score better reflects classification performance. We split the data into \SI{80}{\percent} training and \SI{20}{\percent} validation, using the provided test set for final evaluation.

\subsection{Fine-tuning Pre-trained BERT}
First, we fine-tune a pre-trained BERT model for Named Entity Recognition, using the pre-trained weights. We trained the model for 40 epochs and with three different learning rates ($5e-6$, $10^{-5}$ and $2e-5$). \Cref{fig:task3_bert_results} shows the validation loss and macro F1-score curves for different learning rates: the $10^{-5}$ learning rate achieves the best balance between convergence speed and final performance, with validation loss stabilizing around epoch 20 and F1-score reaching its peak around \num{0.60}. With a lower learning rate ($5e-6$), the model converges slower and achieves a significantly lower macro F1 score. With a higher one ($2e-5$), the initial progress is faster, but the macro F1 score continues rising while the validation loss is already increasing: this may be a symptom of overfitting and poor generalization. Moreover, at 20 epochs it reaches a F1 score similar to the one obtained with $1e-5$. Hence, we choose as best model the one with $1e-5$, stopping the training at 20 epochs.

The performance on the test set is summarized in \cref{tab:task3_bert_comparison}. The model achieves a token accuracy of \num{0.87}. The recall-precision imbalance (\num{0.57} vs {0.84}) indicates that the model is conservative, generating more false negatives than false positives. The average session fidelity is \num{0.82}, indicating that, in general, the model correctly predict $\frac{4}{5}$ tokens in each session. Analyzing the per class F1 scores (\cref{fig:task3_f1_comparison}), we obseve that he model performs well on high-support classes (\emph{Discovery}: 0.94, \emph{Execution}: 0.91) but struggles with minority classes (\emph{Defense Evasion}: 0.40, \emph{Impact}: 0.31, \emph{Other}: 0.25).

\begin{figure}[t]
	\centering
    \subfloat[][{Validation loss.}\label{fig:task3_bert_val}]
	{\includegraphics[width=.48\linewidth]{img/Task3/task3_bert_fine_tuned_val_loss.png}} \quad
	\subfloat[][{Validation macro F1-score.}\label{fig:task3_bert_f1_curve}]
	{\includegraphics[width=.48\linewidth]{img/Task3/task3_bert_fine_tuned_val_macro_f1_scores.png}} \\
  	\caption{Validation curves for pre-trained BERT with different learning rates.}\label{fig:task3_bert_results}
\end{figure}

\subsection{Naked BERT Baseline}
To verify that pre-training is essential, we train a naked BERT model---the same architecture initialized with random weights---from scratch. After grid search ($5\times10^{-6}$, $10^{-5}$, $5\times10^{-5}$), we select the configuration with 25 epochs based on validation F1-score.

\Cref{tab:task3_bert_comparison} compares the results, while \cref{fig:task3_f1_comparison} visualizes the per-class F1 differences. The naked BERT achieves only \SI{74.02}{\percent} token accuracy, a drop of nearly 13 percentage points compared to the pre-trained version. This confirms that SSH command classification is not trivial: the 251 training samples are insufficient for learning from scratch, and pre-trained weights provide essential linguistic knowledge that the naked model cannot discover with such limited data.

\begin{table}
\small
\centering
\caption{Comparison between pre-trained BERT and naked BERT on the test set. Precision, Recall, and F1 are computed using macro-averaging.}
\label{tab:task3_bert_comparison}
\begin{tabular}{lccccc}
\toprule
Model & Token Accuracy & Precision & Recall & F1 score & Avg Fidelity \\
\midrule
Pre-trained BERT  & 0.8694 & 0.8402 & 0.5746 & 0.6399 & 0.8267 \\
Naked BERT        & 0.7402 & 0.5469 & 0.4775 & 0.4925 & 0.7701 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Fine-tuning UniXcoder}
Since UniXcoder was pre-trained on a large code corpus, we hypothesize it has more relevant prior knowledge for SSH command analysis. After grid search ($5\times10^{-6}$, $10^{-5}$, $4\times10^{-5}$), we select the configuration with 15 epochs. \Cref{fig:task3_unixcoder_results} shows the validation curves: the $10^{-5}$ learning rate achieves stable convergence with validation loss decreasing smoothly and F1-score reaching $\approx$0.72, outperforming all BERT configurations.

The hypothesis is confirmed: UniXcoder achieves \SI{89.01}{\percent} token accuracy, outperforming pre-trained BERT by over 2 percentage points. The macro F1-score improves from 0.6399 to 0.6909 (+5.1 points). UniXcoder's specialized vocabulary and representations learned from code corpora enable better handling of shell syntax, operators, and path structures. As shown in \cref{fig:task3_f1_comparison}, UniXcoder is significantly better at identifying \emph{Defense Evasion} samples (F1-score 0.76 vs 0.40 for BERT), likely because code pre-training helps recognize obfuscation patterns.

\begin{figure}[t]
	\centering
    \subfloat[][{Validation loss.}\label{fig:task3_unix_val}]
	{\includegraphics[width=.48\linewidth]{img/Task3/task3_unixcoder_fine_tuned_val_loss.png}} \quad
	\subfloat[][{Validation macro F1-score.}\label{fig:task3_unix_f1_curve}]
	{\includegraphics[width=.48\linewidth]{img/Task3/task3_unixcoder_fine_tuned_val_macro_f1_scores.png}} \\
  	\caption{Validation curves for UniXcoder with different learning rates.}\label{fig:task3_unixcoder_results}
\end{figure}

\subsection{Alternative Fine-tuning Strategies}
We explore parameter-efficient fine-tuning by freezing portions of the UniXcoder encoder. We test two configurations: (1)~training only the last 2 encoder layers + classification head, and (2)~training only the classification head. Full fine-tuning trains all 125.3M parameters, ``Last 2 + Head'' trains 14.18M (\SI{11.31}{\percent}), and ``Head Only'' trains only 5.38k (\SI{0.004}{\percent}).

Full fine-tuning achieves the highest F1-score ($\approx$0.72) with stable convergence. ``Last 2 + Head'' reaches F1~$\approx$0.67, demonstrating that freezing most layers still yields competitive results. ``Head Only'' converges quickly but plateaus at F1~$\approx$0.57, indicating that the frozen encoder cannot fully adapt to the SSH domain.

\paragraph{Performance trade-offs} \Cref{tab:task3_alternative} summarizes the test set results. ``Last 2 + Head'' loses only \SI{1.87}{\percent} accuracy while reducing trainable parameters by \SI{88.69}{\percent}, offering a good efficiency-performance trade-off with 2--3$\times$ training speedup. However, ``Head Only'' loses \SI{8.66}{\percent} accuracy because frozen representations cannot capture domain-specific patterns.

\begin{table}[t]
\small
\centering
\caption{Parameter-efficient fine-tuning comparison for UniXcoder on the test set. Metrics are macro-averaged.}
\label{tab:task3_alternative}
\begin{tabular}{lccc}
\toprule
\textbf{Configuration} & \textbf{Trainable Params.} & \textbf{Token Acc.} & \textbf{F1} \\
\midrule
Full Fine-Tune     & 100.00\%  & 0.8901 & 0.6909 \\
Last 2 + Head      & 11.31\%   & 0.8714 & 0.6521 \\
Head Only          & 0.004\%   & 0.8035 & 0.5628 \\
\bottomrule
\end{tabular}
\end{table}

\Cref{fig:task3_f1_comparison} provides a direct comparison of per-class F1-scores across all model configurations, including support counts for each class. UniXcoder (Full) consistently outperforms both BERT variants on minority classes: \emph{Defense Evasion} improves from 0.40 (BERT) to 0.76 (+36 points), and \emph{Impact} from 0.31 to 0.55 (+24 points). Naked BERT confirms the importance of pre-training, with substantially lower F1-scores across all classes. Progressive freezing degrades performance proportionally: ``Last 2 + Head'' maintains competitive scores on majority classes but drops on \emph{Defense Evasion} (0.59) and \emph{Other} (0.18). ``Head Only'' shows the steepest decline, particularly on \emph{Defense Evasion} (0.24), confirming that adapting encoder representations is essential for recognizing obfuscation patterns.

\begin{figure}[t]
	\centering
	\includegraphics[width=0.9\linewidth]{img/Task3/bert_unixcoder_barplot.png}
	\caption{Per-class F1-scores on the test set for all model configurations, with support counts shown below each class. UniXcoder (Full) achieves the best performance on minority classes, while Naked BERT confirms the importance of pre-training.}
	\label{fig:task3_f1_comparison}
\end{figure}

\subsection{Summary and Best Model Selection}
\Cref{tab:task3_summary} summarizes all models on the test set. The fully fine-tuned UniXcoder achieves the best performance with \SI{89.01}{\percent} token accuracy and \SI{69.09}{\percent} macro F1-score, demonstrating that code-specific pre-training provides a significant advantage for SSH command classification. We select this model (trained for 15 epochs) for the inference task.

\begin{table}[t]
\small
\centering
\caption{Summary of all models on the test set. Precision, Recall, and F1 are macro-averaged. Best results in bold.}
\label{tab:task3_summary}
\begin{tabular}{lcccc}
\toprule
\textbf{Model} & \textbf{Token Acc.} & \textbf{Precision} & \textbf{Recall} & \textbf{F1} \\
\midrule
Pre-trained BERT      & 0.8694 & 0.8402          & 0.5746 & 0.6399 \\
Naked BERT            & 0.7402 & 0.5469          & 0.4775 & 0.4925 \\
UniXcoder (Full)      & \textbf{0.8901} & 0.8359 & \textbf{0.6265} & \textbf{0.6909} \\
UniXcoder (Last 2+H)  & 0.8714 & 0.8043          & 0.5972 & 0.6521 \\
UniXcoder (Head Only) & 0.8035 & 0.7206          & 0.5093 & 0.5628 \\
\bottomrule
\end{tabular}
\end{table}