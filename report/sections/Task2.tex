\section{Tokenization}
In this section, we compare the tokenization behavior of two pre-trained tokenizers: BERT-base (\texttt{google-bert/bert-base-uncased}), a general-purpose model trained on English text, and Unixcoder-base (\texttt{microsoft/unixcoder-base}), a code-oriented model designed for programming languages.

\begin{table}
\small
\centering
\caption{Command tokenization of nine SSH commands with BERT and Unixcoder tokenizers.}
\label{tab:task2_tokenization}
\begin{tabular}{lccccccccc}
\toprule
{Tokenizer} & {\texttt{cat}} & {\texttt{shell}} & {\texttt{echo}} & {\texttt{top}} & {\texttt{chpasswd}} & {\texttt{crontab}} & {\texttt{wget}} & {\texttt{busybox}} & {\texttt{grep}} \\
\midrule
{BERT} & {\texttt{cat}} & {\texttt{shell}} & {\texttt{echo}} & {\texttt{top}} & {\texttt{ch}, \texttt{pass}, \texttt{wd}} & {\texttt{cr}, \texttt{ont}, \texttt{ab}} & {\texttt{w}, \texttt{get}} & {\texttt{busy}, \texttt{box}} & {\texttt{gr}, \texttt{ep}} \\
{Unixcoder} & {\texttt{cat}} & {\texttt{shell}} & {\texttt{echo}} & {\texttt{top}} & {\texttt{ch}, \texttt{passwd}} & {\texttt{c}, \texttt{ront}, \texttt{ab}} & {\texttt{w}, \texttt{get}} & {\texttt{busy}, \texttt{box}} & {\texttt{grep}} \\
\bottomrule
\end{tabular}
\end{table}


\paragraph{Command tokenization} We first tokenize a sample list of nine SSH commands: \texttt{cat}, \texttt{shell}, \texttt{echo}, \texttt{top}, \texttt{chpasswd}, \texttt{crontab}, \texttt{wget}, \texttt{busybox}, and \texttt{grep}. The tokenization is shown in \cref{tab:task2_tokenization}. The BERT tokenizer divides words into subword units based on frequency in general English text, while the Unixcoder tokenizer better understands common shell commands and structures from its code-centric training. Overall, the Unixcoder tokenizer has a better (lower) token/word ratio (\num{1.55}), generating 14 tokens (excluding the \texttt{<s>} and \texttt{<\textbackslash s>} tokens), while BERT has a higher one (\num{1.78}), creating 16 tokens (excluding the \texttt{[CLS]} and \texttt{[SEP]} tokens). Both tokenizers keep certain words intact (e.g., \texttt{cat}, \texttt{echo}, \texttt{shell}) because they represent high-frequency tokens in their respective training data: Unixcoder has many bash commands in its vocabulary from code training, while BERT retains them only if they appear frequently in general text. Other words, instead, are separated by both tokenizers, but in a different manner: while the BERT tokenizer splits \texttt{chpasswd} into three tokens (\texttt{ch}, \texttt{pass}, \texttt{wd}), Unixcoder only into two (\texttt{ch} and \texttt{passwd}), thanks to its superior domain knowledge. The two tokenizers use different markers to denote subword units. BERT prepends \texttt{\#\#} to subsequent tokens of a split word, whereas Unixcoder uses the \texttt{Ä } character as a prefix to indicate the initial token of a word.

\paragraph{Full corpus tokenization} When tokenizing the entire training corpus, BERT produces on average \SI{178.6}{\tokens} per session (with a maximum of \SI{1889}{\tokens}), while Unixcoder \SI{409.3}{\tokens} per session (max \SI{28920}{\tokens}). Both tokenizers produce high token counts because SSH sessions contain many special characters and non-standard strings. BERT generates fewer tokens because it maps unknown or rare character sequences to \texttt{[UNK]} tokens, while Unixcoder, designed for code, attempts to tokenize every character sequence individually. Moreover, Unixcoder performs code-specific splitting on characters like \texttt{\{}, \texttt{\&\&}, and handles file paths by tokenizing each path component separately.

\paragraph{Session truncation} \Cref{fig:task2_ecdf_tokens} shows the ECDF distribution of token counts. For BERT (\cref{fig:task2_ecdf_bert}), approximately \SI{90}{\percent} of sessions falls below the 512-token limit. For Unixcoder (\cref{fig:task2_ecdf_unixcoder}), only about \SI{85}{\percent} of sessions remains below the limit, with a longer tail extending to nearly \num{30000} tokens due to the verbose tokenization of special characters. Specifically, with the 512-token limit, 24 sessions of the training set would be truncated by the BERT tokenizer and 29 by the Unixcoder tokenizer.

\begin{figure}
	\centering
    \subfloat[][{BERT tokenizer.}\label{fig:task2_ecdf_bert}]
	{\includegraphics[width=.45\linewidth]{img/Task2/ecdf_distribution_tokens_bert.png}} \quad
	\subfloat[][{Unixcoder tokenizer.}\label{fig:task2_ecdf_unixcoder}]
	{\includegraphics[width=.45\linewidth]{img/Task2/ecdf_distribution_tokens_unixcode.png}} \\
  	\caption{ECDF distribution of tokens per session. The black line indicates the 512-token context limit.}\label{fig:task2_ecdf_tokens}
\end{figure}

\paragraph{Longest tokenized session}
Both tokenizers produce the maximum number of tokens in the same session, originally composed of 224 words. BERT produces 1889 tokens, of which 113 \texttt{[UNK]} tokens, while Unixcoder produces zero unknown tokens but generates \num{28920} tokens by splitting special characters. The main reason for such a high number of tokens is the content of the session: indeed, it contains a base64 payload composed of more than \num{40000} characters. Both tokenizers interpret it as an exceptionally long sequence of (random) characters, hence failing in tokenizing it properly.

\paragraph{Word truncation preprocessing} Since \SI{98}{\percent} of words are shorter than 30 characters, we truncate all words exceeding this length before tokenization. After this preprocessing, token counts decrease significantly, especially for the Unixcoder tokenizer. Specifically, BERT produces on average \SI{128}{\tokens} per session (max 920), while Unixcoder produces on average \SI{111}{\tokens} per session (max 824). The decrease is more evident for the Unixcoder tokenizer, since it probably attempted to tokenize long base64 payloads, which, instead, are now truncated.

The token-to-word ratios after preprocessing are \num{2.81} for BERT and \num{2.42} for Unixcoder. Unixcoder achieves the better (lower) ratio, indicating more efficient tokenization of SSH commands. \Cref{fig:task2_words_vs_tokens} visualizes this relationship: both plots show a linear correlation between words and tokens. From the comparison of the two plots, we notice that the BERT tokenizer tends to generate slightly more tokens than Unixcoder given the same number of words. Only a few sessions, with more than 200 words, generate a number of tokens higher than the context size. In this case, in fact, only 6 sessions are truncated by both tokenizers.

\begin{figure}[t]
	\centering
    \subfloat[][{BERT Tokenizer.}\label{fig:task2_words_bert}]
	{\includegraphics[width=.45\linewidth]{img/Task2/number_words_vs_number_tokens_bert.png}} \quad
	\subfloat[][{Unixcoder Tokenizer.}\label{fig:task2_words_unixcoder}]
	{\includegraphics[width=.45\linewidth]{img/Task2/number_words_vs_number_tokens_unixcoder.png}} \\
  	\caption{Number of words vs number of tokens per session after word truncation at 30 characters. The black line indicates the 512-token context limit.}\label{fig:task2_words_vs_tokens}
\end{figure}