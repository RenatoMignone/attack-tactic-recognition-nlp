\section{Tokenization}
In this section, we compare the tokenization behavior of two pre-trained tokenizers: BERT-base (\texttt{google-bert/bert-base-uncased}), a general-purpose model trained on English text, and Unixcoder-base (\texttt{microsoft/unixcoder-base}), a code-oriented model designed for programming languages.

\paragraph{Command tokenization} We first tokenize a sample list of nine SSH commands: \texttt{cat}, \texttt{shell}, \texttt{echo}, \texttt{top}, \texttt{chpasswd}, \texttt{crontab}, \texttt{wget}, \texttt{busybox}, and \texttt{grep}. The BERT tokenizer divides words into subword units based on frequency in general English text, while the Unixcoder tokenizer bettew understands common shell commands and structures from its code-centric training. Overall, the Unixcoder tokenizer has a better token/word ratio (\num{1.78}), generating 16 tokens, while Bert has a higher one (\num{2.00}), creating 18 tokens. Both tokenizers keep certain words intact (e.g., \texttt{cat}, \texttt{echo}, \texttt{shell}) because they represent high-frequency tokens in their respective training data: Unixcoder has many bash commands in its vocabulary from code training, while BERT retains them only if they appear frequently in general text. Other words, instead, are separated by both tokenizers, but in a different manner: while the Bert tokenizer splits \texttt{chpasswd} into three tokens (\texttt{ch}, \texttt{pass}, \texttt{wd}), Unixcoder only into two (\texttt{ch} and \texttt{passwd}), thanks to its superior domain knowledge. To indicate that a token was originally part of a longer word, the Bert tokenizer adds \texttt{\#\#} characters; Unixcoder, instead, adds \texttt{Ä } as initial character of the first token of a word.

\paragraph{Full corpus tokenization} When tokenizing the entire training corpus, BERT produces on average \SI{178.6}{\tokens} per session (with a maximum of \SI{1889}{\tokens}), while Unixcoder \SI{409.3}{\tokens} per session (max \SI{28920}{\tokens}). Both tokenizers produce high token counts because SSH sessions contain many special characters and non-standard strings. BERT generates fewer tokens because it maps unknown or rare character sequences to \texttt{[UNK]} tokens, while Unixcoder, designed for code, attempts to tokenize every character sequence individually. Moreover, Unixcoder performs code-specific splitting on characters like \texttt{\{}, \texttt{\&\&}, and handles file paths by tokenizing each path component separately.

\paragraph{Session truncation} \Cref{fig:task2_ecdf_tokens} shows the ECDF distribution of token counts. For BERT (\cref{fig:task2_ecdf_bert}), approximately \SI{90}{\percent} of sessions fall below the 512-token limit. For Unixcoder (\cref{fig:task2_ecdf_unixcoder}), only about \SI{85}{\percent} of sessions remain below the limit, with a longer tail extending to nearly 30000 tokens due to the verbose tokenization of special characters. Specifically, with the 512-token limit, 24 sessions would be truncated by the BERT tokenizer and 29 by the Unixcoder tokenizer.

\begin{figure}
	\centering
    \subfloat[][{BERT tokenizer.}\label{fig:task2_ecdf_bert}]
	{\includegraphics[width=.45\linewidth]{img/Task2/ecdf_distribution_tokens_bert.png}} \quad
	\subfloat[][{Unixcoder tokenizer.}\label{fig:task2_ecdf_unixcoder}]
	{\includegraphics[width=.45\linewidth]{img/Task2/ecdf_distribution_tokens_unixcode.png}} \\
  	\caption{ECDF distribution of tokens per session. The black line indicates the 512-token context limit.}\label{fig:task2_ecdf_tokens}
\end{figure}

\paragraph{Longest tokenized session}
Both tokenizers produce the maximum number of tokens in the same session, originally composed by 224 words. BERT produces 1889 tokens, of which 113 \texttt{[UNK]} tokens, while Unixcoder produces zero unknown tokens but generates 28920 tokens by splitting special characters. The main reason for a such high number of tokens is due to the content of the session: it contains a base64 payload composed by more than \num{40000} characters. Both tokenizers interpret it as an exceptionally long sequence of (random) characters, hence they fail in tokenizing it properly.

\paragraph{Word truncation preprocessing} Since \SI{98}{\percent} of words are shorter than 30 characters, we truncate all words exceeding this length before tokenization. After preprocessing, token counts decrease significantly, especially for the Unixcoder tokenizer. Specifically, BERT produces on average \SI{128}{\tokens} per session (max 920), while Unixcoder produces \SI{111}{\tokens} per session (max 824).

The token-to-word ratios after preprocessing are \num{2.81} for BERT and \num{2.42} for Unixcoder. Unixcoder achieves the better (lower) ratio, indicating more efficient tokenization for SSH commands. \Cref{fig:task2_words_vs_tokens} visualizes this relationship: both plots show a linear correlation between words and tokens. From the comparison of the two plots, we notice that the Bert tokenizer tends to generate slightly more tokens than Unixcoder given the same number of words. Only a few sessions, with more than 200 words, generate a number of tokens higher than the context size. In this case, in fact, only 6 sessions are truncated by both tokenizers.

\begin{figure}[t]
	\centering
    \subfloat[][{BERT Tokenizer.}\label{fig:task2_words_bert}]
	{\includegraphics[width=.45\linewidth]{img/Task2/number_words_vs_number_tokens_bert.png}} \quad
	\subfloat[][{Unixcoder Tokenizer.}\label{fig:task2_words_unixcoder}]
	{\includegraphics[width=.45\linewidth]{img/Task2/number_words_vs_number_tokens_unixcoder.png}} \\
  	\caption{Number of words vs number of tokens per session after word truncation at 30 characters.}\label{fig:task2_words_vs_tokens}
\end{figure}